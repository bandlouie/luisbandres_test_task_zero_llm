{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd71328-abaa-4768-8269-82c8a1b5ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import markdown\n",
    "import warnings\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import IPython.display\n",
    "import datetime as dt\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4ab6d-5c80-46aa-b493-91d4b294ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886b95c-1f14-4a70-b1ea-5cb0200e2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f469b2-52ba-4565-b9e6-173b0d9717ba",
   "metadata": {},
   "source": [
    "## LLM Mapping Chain\n",
    "\n",
    "It doesn't need history for completion. Therefore, is token-efficiente and it doesn't require the memory of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376ab46-b819-4e13-94ed-b656bb1ac67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_template_file_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "\n",
    "You will be provided with a table in a markdown format as << INPUT >>.\n",
    "\n",
    "If there is not a markdown table in the input return an empty JSON object. Otherwise, return a JSON object formatted to look like:\n",
    "\n",
    "<< FORMATTING >>\n",
    "{{{{\n",
    "    \"template_metadata\": [\n",
    "        {{{{\n",
    "            \"header\": string, \\ name of the column. If the input does not contain a header suggest a name for the column based on its data.\n",
    "            \"type\": string, \\ type of the data column of the markdown file in the input. \n",
    "            \"sample\": \\ put a not null samble of the column. This sample should have the most common value which is not null.\n",
    "            \"categorical\": bool \\ check if the column is categorical (true) or not categorical (false).\n",
    "            \"categories_list\": [] \\ list of the unique values if the column is categorical.\n",
    "            \"date_format\": null except if type is date suggest SQL DATE FORMAT for converting the column values to date.\n",
    "            \"description\": string \\ descrption of this column based only on its data.\n",
    "        }}}},\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\n",
    "\n",
    "<< INPUT >>\n",
    "{input_template}\n",
    "\n",
    "<< OUTPUT >>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66e69e-6636-4e2e-8fe4-abb09dcfbac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_file_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "\n",
    "You will be provided with a table in a markdown format as << INPUT >>.\n",
    "Also you will provided with the name of the table as << TABLE >>\n",
    "\n",
    "If there is not a markdown table in the input return an empty JSON object. Otherwise, return a JSON object formatted to look like:\n",
    "\n",
    "<< FORMATTING >>\n",
    "{{{{ \n",
    "    \"table_name\": string, \\ name of the table. you can find it at the begining of the input.\n",
    "    \"file_metadata\": [\n",
    "        {{{{\n",
    "            \"header\": string, \\ name of the column. If the input does not contain a header suggest a name for the column based on its data.\n",
    "            \"type\": string, \\ type of the data column of the markdown file in the input. \n",
    "            \"sample\": \\ put a not null samble of the column. This sample should have the most common value which is not null.\n",
    "            \"date_format\": string, \\ null except if type is date suggest SQL DATE FORMAT for converting the column values to date.\n",
    "            \"description\": string \\ description of this column based only on its data.\n",
    "        }}}},\n",
    "        ...\n",
    "    ],\n",
    "    \"table\" : string \\ the complete markdown table in the input\n",
    "}}}}\n",
    "\n",
    "<< TABLE >>\n",
    "{table_name}\n",
    "\n",
    "<< INPUT >>\n",
    "{input_file}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e5ab3-39b1-47cb-8829-17dc9cb5828f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formating_header_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive two JSON Objects called table_info and template_description as inputs << INPUT >.\n",
    "\n",
    "Follow the following instruction:\n",
    "\n",
    "    Step 1: Go over the list table_info['file_metadata'] and find what is the N header in the list template_description['template_metadata'] most similar to the \"new_file\" table header.\n",
    "    Step 2: return the same \"table_info\" JSON object adding the following information:\n",
    "\n",
    "\"header_match\": [\n",
    "        {{{{\n",
    "            \"table_header\": string, \\ header of \"new_file\" table most similar to the N header of \"template\" table\n",
    "            \"template_header\": string, \\ header of \"template\" table\n",
    "        }}}},\n",
    "        ...\n",
    "    ],\n",
    "\n",
    "\"template_header\" is the N header of \"template\" table most similar to the correspoding header of \"new_file\" table. Determine this similarity based only on the metadata such as:\n",
    "    * Data types\n",
    "    * Samples of both tables\n",
    "    * Description\n",
    "\n",
    "<< INPUT >>\n",
    "{table_info}\n",
    "{template_description}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e043046-b6c4-4347-842a-7d620a918a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_proposal_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive two JSON Objects called table_header_match and template_description as inputs << INPUT >.\n",
    "\n",
    "Follow the next instructions for generating a markdown table:\n",
    "    \n",
    "    Step 1: Go over the list table_header_match[\"header_match\"].\n",
    "    Step 2: For each one of the table_header_match[\"header_match\"][\"table_header\"], replace the header of the markdown table in table_header_match[\"table\"] by table_header_match[\"header_match\"][\"template_header\"]\n",
    "    Step 3: The new markdown table must have only the columns in listed in table_header_match[\"header_match\"][\"template_header\"]. Remove all the remaining columns different to table_header_match[\"header_match\"][\"template_header\"].\n",
    "\n",
    "Return the new markdown table in a JSON Object formatted to look like:\n",
    "\n",
    "<< FORMATTING >>\n",
    "{{{{ \n",
    "    \"table_name\": string, \\ name of the table. you can find it in table_header_match[\"table_name\"].\n",
    "    \"file_metadata\": [\n",
    "        {{{{\n",
    "            \"header\": string, \\ name of the column.\n",
    "            \"type\": string, \\ type of the data column of the markdown file in the input. \n",
    "            \"sample\": \\ put a not null samble of the column. This sample should have the most common value which is not null.\n",
    "            \"categorical\": bool \\ check if the column is categorical (true) or not categorical (false).\n",
    "            \"categories_list\": [] \\ list of the unique values if the column is categorical.\n",
    "            \"date_format\": null except if type is date suggest SQL DATE FORMAT for converting the column values to date.\n",
    "            \"description\": string \\ descrption of this column based only on its data.\n",
    "        }}}},\n",
    "        ...\n",
    "    ],\n",
    "    \"modified_table\" : string, \\ the new markdown table.\n",
    "    \"template_metadata\": template_description[\"template_metadata\"]  \\ the template metadata.\n",
    "}}}}\n",
    "\n",
    "<< INPUT >>\n",
    "{table_header_match}\n",
    "{template_description}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78000db-e16d-4dfa-836b-2e17203b1c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formating_categories_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive a JSON Object called simple_table as input << INPUT >.\n",
    "\n",
    "Based on the given input, the task is to find the column in the \"modified_table\" that is most similar to the N header in the \"template_metadata\" list. Then, select only the categorical columns and return the \"simple_table\" JSON object with the added information.\n",
    "\n",
    "Follow the next instructions for generating a markdown table:\n",
    "Step 1: compare the headers in the \"modified_table\" with the headers in the \"template_metadata\" list. We will  iterate over the columns in the \"modified_table\" and find the column that is most similar to the N header in the \"template_metadata\" list.\n",
    "Step 2: After finding the most similar column, check if it is a categorical column by checking the \"categorical\" key in the \"file_metadata\" list.\n",
    "Step 3: Add the following information to the \"simple_table\" JSON object.\n",
    "Step 4: Return the updated  \"simple_table\" JSON object.\n",
    "\n",
    "\"categories_match\": [\n",
    "    {{{{\n",
    "        \"categories_list\": string, \\ list of categories in simple_table['template_metadata']\n",
    "        \"table_header\": string, \\ header of markdown table in simple_table[\"modified_table\"] most similar to the N header of \"template_metadata\"\n",
    "    }}}},\n",
    "    ...\n",
    "],\n",
    "\n",
    "<< INPUT >>\n",
    "{simple_table}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Return the updated  \"simple_table\" JSON object. Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4766f-7d4a-4b96-90c5-178313553147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories_result_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive a JSON Object called table_categories_match as input << INPUT >.\n",
    "\n",
    "Based on the given input, the task is to generate a markdown table by replacing each value in the categorical columns of the \"modified_table\" with the most similar item from the \"categories_list\" in the \"categories_match\" section. The updated table should be returned as a JSON object.\n",
    "\n",
    "Follow the next instructions for generating a markdown table:\n",
    "Step 1: Iterate over each categorical column in the \"modified_table\".\n",
    "Step 2: Replace each value in the column with the most similar item from the \"categories_list\" in the \"categories_match\" section.\n",
    "Setp 3: the new markdown table will be called correct_cats_markdown_table\n",
    "\n",
    "Return the new markdown table (correct_cats_markdown_table) in a JSON Object formatted to look like:\n",
    "\n",
    "<< FORMATTING >>\n",
    "{{{{ \n",
    "    \"table_name\": string, \\ name of the table. you can find it in table_categories_match[\"table_name\"].\n",
    "    \"file_metadata\": [\n",
    "    {{{{\n",
    "        \"header\": string, \\ name of the column.\n",
    "        \"type\": string, \\ type of the data column of the markdown file in the input. \n",
    "        \"sample\": \\ put a not null samble of the column. This sample should have the most common value which is not null.\n",
    "        \"categorical\": bool \\ check if the column is categorical (true) or not categorical (false).\n",
    "        \"categories_list\": [] \\ list of the unique values if the column is categorical.\n",
    "        \"date_format\": null except if type is date suggest SQL DATE FORMAT for converting the column values to date.\n",
    "        \"description\": string \\ descrption of this column based only on its data.\n",
    "    }}}},\n",
    "    ...\n",
    "    ],\n",
    "    \"table\" : string, \\ the new markdown table (correct_cats_markdown_table).\n",
    "    \"template_metadata\": table_categories_match[\"template_metadata\"]  \\ the template metadata.\n",
    "}}}}\n",
    "\n",
    "<< INPUT >>\n",
    "{table_categories_match}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Return only the JSON Object. Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c545e-ae06-48fc-967f-99f5b998004b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formating_dates_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive a JSON Object called table_categories_result as input << INPUT >.\n",
    "\n",
    "Change the format of each one the rows of date columns in the markdown in table_categories_result[\"table\"] according to the date format in the list table_categories_result[\"template_metadata\"]\n",
    "\n",
    "The new markdown table will be called correct_dates_markdown_table\n",
    "\n",
    "Return the new markdown table (correct_dates_markdown_table) in a JSON Object formatted to look like:\n",
    "\n",
    "<< FORMATTING >>\n",
    "{{{{ \n",
    "    \"table_name\": string, \\ name of the table. you can find it in table_categories_result[\"table_name\"].\n",
    "    \"table\" : string, \\ the new markdown table (correct_dates_markdown_table).\n",
    "    \"template_metadata\": table_categories_result[\"template_metadata\"]  \\ the template metadata.\n",
    "}}}}\n",
    "\n",
    "<< INPUT >>\n",
    "{table_categories_result}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Return only the JSON Object. Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eae172-ad8b-48dd-b874-916a2bc0948b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formating_strings_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will receive a JSON Object called table_dates_result as input << INPUT >.\n",
    "\n",
    "Based on the given input, the task is to find the column in the markdown \"table\" that is most similar to the N header in the \"template_metadata\" list. Then, select only the string columns and return the \"table_dates_result\" JSON object with the added information.\n",
    "\n",
    "Follow the next instructions for generating a markdown table:\n",
    "Step 1: compare the headers in the \"table\" with the headers in the \"template_metadata\" list. We will  iterate over the columns in the \"table\" and find the column that is most similar to the N header in the \"template_metadata\" list.\n",
    "Step 2: After finding the most similar column, check if it is a string column by checking the \"type\" key in the \"file_metadata\" list.\n",
    "Step 3: Ignore if it is a categorical, numerical or date column by checking the \"categorical\" key in the \"file_metadata\" list.\n",
    "Step 4: Add the following information to the \"table_dates_result\" JSON object.\n",
    "Step 5: Transform all the rows of string columns of markdown table so they look like than their columns in \"template_metadata\".\n",
    "Step 5: Return the updated  \"table_dates_result\" JSON object.\n",
    "\n",
    "\"strings_match\": [\n",
    "    {{{{\n",
    "        \"selected_sample\": string, \\ sample of data in table_dates_result[\"template_metadata\"]\n",
    "        \"table_header\": string, \\ header of markdown table in table_dates_result[\"table\"] most similar to the N header of \"template_metadata\"\n",
    "    }}}},\n",
    "    ...\n",
    "],\n",
    "\n",
    "<< INPUT >>\n",
    "{table_dates_result}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Return the updated \"table_dates_result\" JSON object. Only return a JSON Object, no more!! The only valid output is a JSON Object.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad655a7-5623-47d2-a44f-0dc1c0dcefce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_template_load = LLMChain(llm=llm, prompt=load_template_file_prompt, \n",
    "                     output_key=\"template_description\"\n",
    "                    )\n",
    "chain_load = LLMChain(llm=llm, prompt=load_file_prompt, \n",
    "                     output_key=\"table_info\"\n",
    "                    )\n",
    "chain_header_formatting = LLMChain(llm=llm, prompt=formating_header_prompt, \n",
    "                     output_key=\"table_header_match\"\n",
    "                    )\n",
    "chain_proposal = LLMChain(llm=llm, prompt=table_proposal_prompt, \n",
    "                     output_key=\"simple_table\"\n",
    "                    )\n",
    "chain_cats_formatting = LLMChain(llm=llm, prompt=formating_categories_prompt, \n",
    "                     output_key=\"table_categories_match\"\n",
    "                    )\n",
    "chain_cats_result = LLMChain(llm=llm, prompt=categories_result_prompt, \n",
    "                     output_key=\"table_categories_result\"\n",
    "                    )\n",
    "chain_dates_result = LLMChain(llm=llm, prompt=formating_dates_prompt, \n",
    "                     output_key=\"table_dates_result\"\n",
    "                    )\n",
    "chain_strings_formatting = LLMChain(llm=llm, prompt=formating_strings_prompt, \n",
    "                     output_key=\"table_strings_match\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118147fe-30ab-40d3-8f5c-898d25bc481d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_chain = SequentialChain(\n",
    "    chains=[chain_template_load, chain_load, chain_header_formatting, chain_proposal, chain_cats_formatting, chain_cats_result, chain_dates_result, chain_strings_formatting],\n",
    "    input_variables=[\"input_template\",\"table_name\",\"input_file\"],\n",
    "    output_variables=[\"table_header_match\",\"table_categories_match\",\"table_strings_match\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8edb3-de6f-4a82-b550-e9c4c8ac6a7f",
   "metadata": {},
   "source": [
    "## LLM Coding Chain\n",
    "\n",
    "It doesn't need history for completion. Therefore, is token-efficiente and it doesn't require the memory of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddecb26-abe6-4f97-856e-450b8504ddac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_data_attributes(analysis_chain):\n",
    "    return dict(\n",
    "        # Inputs\n",
    "        template_table = analysis_chain['input_template'],\n",
    "        initial_table = analysis_chain['input_file'],\n",
    "        # Metadata\n",
    "        file_metadata = json.loads(analysis_chain['table_header_match'])['file_metadata'],\n",
    "        template_metadata = json.loads(analysis_chain['table_categories_match'])['template_metadata'],\n",
    "        # Feature Mappings\n",
    "        header_match = json.loads(analysis_chain['table_header_match'])['header_match'],\n",
    "        categories_match = json.loads(analysis_chain['table_categories_match'])['categories_match'],\n",
    "        strings_match = json.loads(analysis_chain['table_strings_match'])['strings_match'],\n",
    "        # Final Table\n",
    "        final_table = json.loads(analysis_chain['table_strings_match'])['table']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfc56f-6c99-480c-9958-e2a71da416f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_python_code_prompt():\n",
    "    global execution_chain\n",
    "    # try:\n",
    "    map_process = extract_data_attributes(execution_chain)\n",
    "    cat_list = sum([d['categories_list'] for d in map_process['categories_match']],[])\n",
    "    cat_headers = ', '.join([d['table_header'] for d in map_process['categories_match'] if len(d['categories_list'])>0])\n",
    "    return f\"\"\"\n",
    "    You will be provided with a initial_table in a markdown format as << INITIAL_TABLE >>.\n",
    "    You will be provided with a template_table in a markdown format as << TEMPLATE_TABLE >>.\n",
    "    You will be provided with a JSON object with headers mapping as << HEADERS MAPPING >>>\n",
    "    You will be provided with a list of allowed categories as << CATEGORIES ALLOWED >>>\n",
    "\n",
    "    Create a python code for transforming the initial_table into template_table so initial_table will be indetical to template_table. Python Code must handle exceptions at each step: Python Code must end without errors.\n",
    "    \n",
    "    initial_table must be loaded from csv file as a dataframe of only strings using pandas 1.3.1. and python 3.9. change name to dataframe.\n",
    "    \n",
    "    template_table is only a markdown (is not a csv file) that only exists in this prompt as a guide.\n",
    "    \n",
    "    Headers must be renamed according to << HEADERS MAPPING >> \n",
    "    \n",
    "    All the rows of columns of renamed dataframe must look like than their columns in template_table: must have the same punctuation and letter cases\n",
    "    \n",
    "    Transform all the rows of columns (for serials) of renamed dataframe so they look like than their columns (for serials) in template_table.\n",
    "    \n",
    "    Transform the string columns with dates in dataframe to have the same date format than template_table. Consider the previous steps.\n",
    "    \n",
    "    Replace each value in the categories columns ({cat_headers}) with the most similar (difflib.get_close_matches()) item from the list in << CATEGORIES ALLOWED >>>. When calculate similarity, not use index [0] if difflib.get_close_matches() returns an empty list. In that case use the original category value. All resulting categories columns must be string columns. The python code needs to replace each value in the categorical columns of the renamed dataframe with the most similar item from list in the << CATEGORIES ALLOWED >>>. All categories must be kept as strings always.\n",
    "    \n",
    "    Only keep the same columns than template_table.\n",
    "\n",
    "    Save the dataframe as csv file called \"transformed_table\".\n",
    "\n",
    "    << INITIAL_TABLE >>\n",
    "    ```markdown\n",
    "    {json.dumps(map_process['initial_table'])}\n",
    "    ```\n",
    "\n",
    "    << TEMPLATE_TABLE >>\n",
    "    ```markdown\n",
    "    {json.dumps(map_process['final_table'])}\n",
    "    ```\n",
    "        \n",
    "    << HEADERS MAPPING >>\n",
    "    ```json\n",
    "    {json.dumps(map_process['header_match'])}\n",
    "    ```\n",
    "    \n",
    "    << CATEGORIES ALLOWED >>>\n",
    "    ```json\n",
    "    {json.dumps(cat_list)}\n",
    "    ```\n",
    "\n",
    "    << OUTPUT >>\n",
    "    You must return only a complete python script. Please avoid make extra comments, I need only the python script.\n",
    "\n",
    "    \"\"\"\n",
    "    # except Exception as e:\n",
    "    #     print(f\"{e}\")\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb7075-af62-4ef0-87eb-b2811660a7a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## User Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2f494-a8b0-44d3-a13a-32e85bd37eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_to_html(md_table_string):\n",
    "    return markdown.markdown(md_table_string, extensions=['markdown.extensions.tables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18425fa-deb0-4aee-96fb-3275c75b7121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_csv(file, file_label):\n",
    "    global _file_buffer\n",
    "    \n",
    "    # Read the uploaded CSV file with pandas\n",
    "    df = pd.read_csv(io.StringIO(file.decode('utf-8')))\n",
    "    \n",
    "    # Convert the DataFrame to an HTML table with added styles\n",
    "    html_table = df.to_html(classes='table table-striped')\n",
    "    \n",
    "    # Add CSS for scrollable table\n",
    "    styled_table = f\"\"\"\n",
    "    <div style=\"max-width: 100%; overflow-x: auto;\">\n",
    "        {html_table}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    _file_buffer[file_label] = df.to_markdown()\n",
    "    \n",
    "    return styled_table\n",
    "\n",
    "def process_template(file):\n",
    "    return process_csv(file, 'template')\n",
    "\n",
    "def process_new_file(file):\n",
    "    return process_csv(file, 'new_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533c6cc-1922-45af-be7b-a23d2f6d429e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_file_buffer = {\n",
    "    'template':'',\n",
    "    'new_file':'',\n",
    "}\n",
    "execution_chain = None\n",
    "def tables_analysis():\n",
    "    global _file_buffer\n",
    "    global execution_chain\n",
    "    execution_chain = mapping_chain({\n",
    "        \"input_template\":_file_buffer['template'],\n",
    "        \"table_name\":\"new_file\",\n",
    "        \"input_file\":_file_buffer['new_file']\n",
    "    })\n",
    "    result_chain = json.loads(execution_chain['table_strings_match'])\n",
    "    show_table_html = markdown_to_html(result_chain['table'])\n",
    "    system_context = f\"\"\"\n",
    "This is the transformed data according to the template.\n",
    "\n",
    "Transformed Data:\n",
    "\n",
    "{result_chain['table']}\n",
    "\n",
    "Template:\n",
    "\n",
    "{execution_chain['input_template']}\n",
    "\n",
    "Input File:\n",
    "\n",
    "{execution_chain['input_file']}\n",
    "    \"\"\"\n",
    "    return show_table_html, system_context\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3a904-bbd0-4e5a-adfd-2efa461b0897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anaylisis_check = False\n",
    "def feedback_analysis(res):\n",
    "    global anaylisis_check\n",
    "    anaylisis_check = (res=='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78da36-b132-4b5a-b520-3de7803b4963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python_text = ''\n",
    "is_new_chat = True\n",
    "def generate_python_code():\n",
    "    global python_text\n",
    "    global is_new_chat\n",
    "    if anaylisis_check:\n",
    "        try:\n",
    "            del python_code_conv\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            # No uses memory\n",
    "            python_code_conv = ConversationChain(\n",
    "                llm=llm, \n",
    "                verbose=False\n",
    "            )\n",
    "            python_text = python_code_conv.predict(input=get_python_code_prompt())\n",
    "            python_text = python_text.split('```python')[1].split('```')[0]\n",
    "            is_new_chat = True\n",
    "        except:\n",
    "            python_text = \"\"\n",
    "    else:\n",
    "        python_text = \"Please confirm the file was mapped correctly.\"\n",
    "    return python_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb5fe5-9add-4443-8411-54121c1898d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python_code_check = False\n",
    "def feedback_python_code(res):\n",
    "    global anaylisis_check\n",
    "    global python_code_check\n",
    "    python_code_check = (res=='Yes')\n",
    "    if anaylisis_check:\n",
    "        if python_code_check:\n",
    "            return \"Python Code Valid!\"\n",
    "        else:\n",
    "            return \"Python Code Invalid!\"\n",
    "    else:\n",
    "        return \"Please confirm Analysis at Step 2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31e73b-2679-4ab0-a682-072936d48d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_sample():\n",
    "    global python_text\n",
    "    if (not (python_text is None)) & (python_text!=''):\n",
    "        task_date_tag = dt.datetime.strftime(dt.datetime.now(), '%Y%m%d_%H%M%S')\n",
    "        with open(f'./additional_task/training_data/sample_{task_date_tag}.pickle', 'wb') as handle:\n",
    "            pickle.dump({\n",
    "                'prompt':get_python_code_prompt(),\n",
    "                'completion':python_text\n",
    "            }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return 'Sample saved'\n",
    "    else:\n",
    "        return 'Python script must be generated first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4e857-1aa6-4244-94b0-6dfc40eb1a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_python_code():\n",
    "    global anaylisis_check\n",
    "    global python_code_check\n",
    "    global python_text\n",
    "    if (anaylisis_check & python_code_check):\n",
    "        # Save the string to a file\n",
    "        filename = \"output_python_code.py\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(python_text)\n",
    "\n",
    "        # Return the file path so Gradio can allow the user to download it\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69d5f6-6cee-4c14-997c-1a042a44b397",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additonal Task Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac3088-14b4-4269-88b5-6f630b95820f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_string_output = ''\n",
    "def start_training_model():\n",
    "    global run_string_output\n",
    "    # Joining all training samples\n",
    "    try:\n",
    "        training_data = []\n",
    "        for file_name in os.listdir(\"./additional_task/training_data/\"):\n",
    "            if file_name.endswith(\".pickle\"):\n",
    "                with open(f'./additional_task/training_data/{file_name}', 'rb') as handle:\n",
    "                    training_data.append(pickle.load(handle))\n",
    "        training_data = [\n",
    "            {'prompt':d['prompt']+'\\n\\n###\\n\\n',\n",
    "             'completion':d['completion']+' END'\n",
    "            } for d in training_data\n",
    "        ]\n",
    "\n",
    "        with open(f'./additional_task/training_file.jsonl', 'w') as handle:\n",
    "            handle.write(json.dumps(training_data)[1:-1])\n",
    "    except Exception as e:\n",
    "        return f'[ERROR] Failed to load training data: {e}'\n",
    "    \n",
    "    # Preparing Data\n",
    "    \n",
    "    try:\n",
    "        subprocess.check_output(\n",
    "            ['rm','./additional_task/training_file_prepared.jsonl']\n",
    "        )\n",
    "        run_string_output = subprocess.check_output(\n",
    "            ['openai','tools','fine_tunes.prepare_data','-f','./additional_task/training_file.jsonl','-q']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"{e}\"\n",
    "        \n",
    "    # Starting Training Job\n",
    "    try:\n",
    "        run_string_output = subprocess.check_output(\n",
    "            ['openai','api','fine_tunes.create','-t','./additional_task/training_file_prepared.jsonl','--no_check_if_files_exist','-m','ada:ft-personal-2023-08-16-15-52-42','--n_epochs','10']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        run_string_output = f\"{e}\"\n",
    "\n",
    "    try:\n",
    "        run_string_output = run_string_output.decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return run_string_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8a7a3-bfab-484b-a527-803e4e2cbbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_status():\n",
    "    global run_string_output\n",
    "    try:\n",
    "        train_job_id = run_string_output.split('openai api fine_tunes.follow -i ')[1].strip()\n",
    "        return subprocess.check_output(['openai','api','fine_tunes.follow','-i',train_job_id]).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df465c-ab5b-4f93-b7bf-78f7b14eaa5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_list = ['Press Refresh Button']\n",
    "def get_models_list():\n",
    "    global models_list\n",
    "    try:\n",
    "        models_list =  subprocess.check_output(['openai','api','fine_tunes.list']).decode('utf-8')\n",
    "\n",
    "        models_list = pd.DataFrame(json.loads(models_list)['data'])\n",
    "        models_list = models_list[['id','updated_at','model','fine_tuned_model']]\n",
    "\n",
    "        models_list = models_list.sort_values(by='updated_at',ascending=False)\n",
    "\n",
    "        models_list = models_list['fine_tuned_model'].tolist()\n",
    "        \n",
    "        models_list = [m for m in models_list if not (m is None)]\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        models_list = ['Refresh the UI']\n",
    "    \n",
    "    return gr.Dropdown.update(choices=models_list)\n",
    "_ = get_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dc7ee-9f71-4a69-882f-36a67d164abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Chatbot functions\n",
    "There is a chatbot, here GPT memory handle the token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb61bb0-2f0a-4803-872c-7c404790eef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=4000)\n",
    "bot_conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=False,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0d88b-f0a3-43c4-9de2-a68a55bdc9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def respond(message, chat_history, instruction, temperature=0.0):  \n",
    "    global is_new_chat\n",
    "    try:\n",
    "        if (python_text is None) | (python_text==''):\n",
    "            chat_history.append((message, 'You need to Generate the Python Code in Step 3. If it does, press again \"Generate\" in Step 3'))\n",
    "            return message, chat_history\n",
    "        if is_new_chat & (not (python_text is None)) & (python_text!=''):\n",
    "            memory.save_context(inputs = {'prompt':get_python_code_prompt()},\n",
    "                                outputs={'completion':python_text})\n",
    "            is_new_chat = False\n",
    "        bot_message = bot_conversation.predict(input=message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        chat_history.append((message, 'Somehting went wrong. Refresh your browser. If the issue persists, restart the app.'))\n",
    "        return message, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f675b-4964-42d2-a444-a5ad8fce808f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def respond_wrapper(message, chat_history, instruction, temperature=0.0):\n",
    "    return respond(message, chat_history, instruction, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f364f-18ec-4138-a762-0e85ea582488",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Addtional Task: Fine-Tuning functions\n",
    "There is the completion task for using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78311f-4ae9-4154-adcd-7695f450d076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_model_llm = None\n",
    "def load_fine_tuned_model_bot(model_name):\n",
    "    global selected_model_llm\n",
    "    global python_text\n",
    "    try:\n",
    "        selected_model_llm = OpenAI(model=model_name, temperature=0.0)\n",
    "        return model_name\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        selected_model_llm = None\n",
    "        return f\"[ERROR] Not selected model {model_name} or invalid model: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d7c24-6465-4e42-8fcd-e6f0535e7cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_completion = ''\n",
    "def fine_tuned_completion():\n",
    "    global text_completion\n",
    "    try:\n",
    "        prompt_load = get_python_code_prompt()\n",
    "        if text_completion == '':\n",
    "            text_completion = (\"=\"*20) + \"\\nPROMPT\\n\" + (\"=\"*20) + \"\\n\\n\" + prompt_load + \"\\n\\n\" + (\"=\"*20) + \"\\nCOMPLETION\\n\" + (\"=\"*20) + \"\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return \"You need to Transform Table First (Step 2).\"\n",
    "    try:\n",
    "        text_for_llm = [t for t in text_completion.split(' ') if len(t.replace(' ',''))>0]\n",
    "        text_for_llm = ' '.join(text_for_llm[-1900:])\n",
    "        generated_completion = selected_model_llm(text_for_llm)\n",
    "        text_completion = text_completion + '\\n\\n' + generated_completion\n",
    "        return text_completion\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return \"You need to choose a model first!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6ad9c-e8df-4914-81c8-99460c1a6e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tuned_clear_completion():\n",
    "    global text_completion\n",
    "    text_completion = ''\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578b251-81c0-42f3-9fac-30258567427c",
   "metadata": {},
   "source": [
    "### Gradio Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685d86e-7b96-426c-bfd1-7c2b586bfad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.HTML('<h1 align=\"center\">Test Task Submission</h1>')\n",
    "            gr.HTML('<h2 align=\"center\">Luis Bandres</h2>')\n",
    "            gr.HTML('<p align=\"center\">Add description here</p>')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # Load Data\n",
    "            gr.HTML('<h2 align=\"center\">Step 1: Load Data</h2>')\n",
    "            \n",
    "            upload_template = gr.inputs.File(type=\"bytes\", label=\"Upload Template\")\n",
    "            data_template = gr.outputs.HTML(label=\"Template\")\n",
    "            upload_template.upload(process_template, inputs=upload_template, outputs=data_template)\n",
    "            \n",
    "            upload_file = gr.inputs.File(type=\"bytes\", label=\"Upload New File\")\n",
    "            data_file = gr.outputs.HTML(label=\"New File\")\n",
    "            upload_file.upload(process_new_file, inputs=upload_file, outputs=data_file)\n",
    "\n",
    "        with gr.Column():\n",
    "            # Analyse Data\n",
    "            gr.HTML('<h2 align=\"center\">Step 2: Transform using LLM </h2>')\n",
    "            gr.HTML('<h3 align=\"left\">Dont leave this page while processing.</h3>')\n",
    "            gr.HTML('<p align=\"left\">This process could take 5 minutes approximately...</p>')\n",
    "            btn_analyse = gr.Button(\"Transform Table\")\n",
    "            data_proposal = gr.outputs.HTML(label=\"Data Mapping Result\")\n",
    "            chk_analysis = gr.Radio([\"Yes\", \"No\"], label=\"Data was mapped correctly?\")\n",
    "\n",
    "            # Generating Code\n",
    "            gr.HTML('<h2 align=\"center\">Step 3: Generate Python Code </h2>')\n",
    "            btn_python_code = gr.Button(\"Generate\")\n",
    "            text_python_code = gr.Textbox(value=\"Please Generate Python Code\",label=\"Python Code\")\n",
    "            chk_python_code = gr.Radio([\"Yes\", \"No\"], label=\"Python code is correct?\")\n",
    "            \n",
    "            # Saving Training Data\n",
    "            gr.HTML('<h2 align=\"center\">Step 4: Saving Training Data </h2>')\n",
    "            gr.HTML('<p align=\"left\">This button store the prompt for creating the python code and the generated script.</p>')\n",
    "            gr.HTML('<p align=\"left\">This sample will be used for fine tuning a Davinci Model (gpt 3.5) in OpenAi (Step 7).</p>')\n",
    "            btn_save_sample = gr.Button(\"Save Sample\")\n",
    "            text_save_sample_result = gr.Textbox(label=\"Save Sample Result\")\n",
    "            \n",
    "            # Edit Code\n",
    "            gr.HTML('<h2 align=\"center\">Step 5: Download Python Code </h2>')\n",
    "            gr.HTML('<h3 align=\"left\">Requisites:</h3>')\n",
    "            gr.HTML('<p align=\"left\">   * Step 2 must be confirmed.</p>')\n",
    "            gr.HTML('<p align=\"left\">   * Step 3 must be confirmed.</p>')\n",
    "            python_code_result = gr.outputs.HTML(label=\"Result Python Code\")\n",
    "            btn_download_python = gr.Button(\"Save Code\")\n",
    "            download_python = gr.outputs.File(label=\"Generated Python Code\")\n",
    "    \n",
    "    with gr.Row():    \n",
    "        with gr.Column():\n",
    "            # Chatbot\n",
    "            gr.HTML('<h2 align=\"center\">Step 6: Advanced Options </h2>')\n",
    "            gr.HTML('<b align=\"left\">This is a chatbot powered by OpenAI GPT 3.5 so it can help you to editing the code with AI Assistance. The Table Analysis and the Generated Python Code have been loaded to this assistant.</p>')\n",
    "            chatbot = gr.Chatbot(height=446, label='Chatbot') #just to fit the notebook\n",
    "            msg = gr.Textbox(label=\"Prompt\")\n",
    "            with gr.Accordion(label=\"Settings\",open=False):\n",
    "                system_context = gr.Textbox(label=\"System Context\", lines=2, value=\"A conversation between a user and an LLM-based AI python coding assistant. The assistant gives helpful, honest, and precise answers. The assistant must act as a programmer.\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    btn = gr.Button(\"Submit\")\n",
    "                with gr.Column():\n",
    "                    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # Addditional Task\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.HTML('<br><br><hr class=\"solid\"><br><br>')\n",
    "            gr.HTML('<h2 align=\"center\">Step 7: Additional Task. Use Fine-Tuned Model </h2>')\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.HTML('<h2 align=\"center\">Step 7.1: Training Model</h2>')\n",
    "                    btn_start_train = gr.Button(\"Start Training Model\")\n",
    "                    btn_follow_train = gr.Button(\"Get Training Status\")\n",
    "                    text_follow_train_result = gr.Textbox(label=\"Training Status\")\n",
    "                with gr.Column():\n",
    "                    gr.HTML('<h2 align=\"center\">Step 7.2: Loading Model</h2>')\n",
    "                    btn_refresh_models = gr.Button(\"Refresh Models List\")\n",
    "                    models_dropdown = gr.Dropdown(models_list,multiselect=False,label='Fine-Tuned LLM Model')\n",
    "                    btn_use_model = gr.Button(\"Use This Model\")\n",
    "                    text_model_select_result = gr.Textbox(label=\"Model Selection Status\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.HTML('<h2 align=\"center\">Step 7.3: Make Inferences</h2>')\n",
    "            gr.HTML('<p> This task uses the selected Fine-Tuned Model for completing the prompt used in Step 3 for Generated Python Code</p>')\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.HTML('<p> Click on \"Make Completion\" any time you want for comleting the texts.</p>')\n",
    "                    btn_model_completion = gr.Button(\"Make Completion\")\n",
    "                    btn_clear_completion = gr.Button(\"Clear\")\n",
    "                with gr.Column():\n",
    "                    text_model_completion = gr.Textbox(label=\"Completion Result\",lines=30)\n",
    "            \n",
    "    # Actions\n",
    "    \n",
    "    # Transform tables\n",
    "    btn_analyse.click(tables_analysis, inputs=None, outputs=[data_proposal,system_context])\n",
    "    chk_analysis.change(feedback_analysis,inputs=chk_analysis, outputs=None)\n",
    "    \n",
    "    # Generate python code\n",
    "    btn_python_code.click(generate_python_code,inputs=None,outputs=text_python_code)\n",
    "    chk_python_code.change(feedback_python_code,inputs=chk_python_code, outputs=python_code_result)\n",
    "\n",
    "    btn_download_python.click(download_python_code,inputs=None,outputs=download_python)\n",
    "    \n",
    "    # Save training samples\n",
    "    btn_save_sample.click(save_training_sample,inputs=None,outputs=text_save_sample_result)\n",
    "    \n",
    "    # Chatbot (Advanced Options)\n",
    "    btn.click(respond_wrapper, inputs=[msg, chatbot, system_context], outputs=[msg, chatbot])\n",
    "    msg.submit(respond_wrapper, inputs=[msg, chatbot, system_context], outputs=[msg, chatbot])\n",
    "    \n",
    "    # Fine-Tuning: Start and Monitoring Training Jobs\n",
    "    btn_start_train.click(start_training_model,inputs=None,outputs=text_follow_train_result)\n",
    "    btn_follow_train.click(get_training_status,inputs=None,outputs=text_follow_train_result)\n",
    "    \n",
    "    # Fine-Tuning: Selecting Models\n",
    "    btn_refresh_models.click(get_models_list,inputs=None,outputs=None)\n",
    "    btn_use_model.click(load_fine_tuned_model_bot,inputs=models_dropdown,outputs=text_model_select_result)\n",
    "    \n",
    "    # Fine-Tuning: Make completion\n",
    "    btn_model_completion.click(fine_tuned_completion,inputs=None,outputs=text_model_completion)\n",
    "    btn_clear_completion.click(fine_tuned_clear_completion,inputs=None,outputs=text_model_completion)\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=False, server_port=int(os.environ['GRADIO_SERVER_PORT']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67deb224-8669-475b-b759-49dbf3d4acc6",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
